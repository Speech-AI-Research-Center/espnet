{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESPnet-Easy with torchaudio.datasets\n",
    "This Jupyter notebook provides a step-by-step guide on using the `torchaudio.datasets` library as an external dataset with ESPnet-Easy. In this demonstration, we will leverage the LibriSpeech dataset to train an Automatic Speech Recognition (ASR) model using the Librispeech-100 subset.\n",
    "\n",
    "Prior to executing the code in this notebook, it is essential to install the torchaudio library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "To obtain the dataset, we can refer to the official documentation of [torchaudio.datasets](https://pytorch.org/audio/stable/datasets.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import espnetez as ez\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "    root='/hdd/database/torchaudio',\n",
    "    url='train-clean-100',\n",
    "    download=True\n",
    ")\n",
    "valid_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "    root='/hdd/database/torchaudio',\n",
    "    url='test-clean',\n",
    "    download=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentencepiece model\n",
    "\n",
    "To train a SentencePiece model, we need a text file for training. \n",
    "\n",
    "Let's begin by creating the training file, and then execute `ez.preprocess.train_sentencepiece` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training texts from the training data\n",
    "# you can select several datasets to train sentencepiece.\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "SPM_DIR = \"data/spm\"\n",
    "SPM_TRAIN_FILE = f\"{SPM_DIR}/train.txt\"\n",
    "\n",
    "if not os.path.exists(SPM_DIR):\n",
    "    os.makedirs(SPM_DIR)\n",
    "\n",
    "# create training data\n",
    "text = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    text.append(train_dataset[i][2])  # get transcription\n",
    "\n",
    "with open(SPM_TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write('\\n'.join(text))\n",
    "\n",
    "ez.preprocess.train_sentencepiece(\n",
    "    SPM_TRAIN_FILE,\n",
    "    \"data/bpemodel\",\n",
    "    vocab_size=5000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap with ESPnetEasyDataset\n",
    "\n",
    "Before initiating the training process, it is crucial to adapt the dataset to the ESPnet format. The dataset class should output tokenized text and audio files in `np.array` format.\n",
    "\n",
    "The `torchaudio.datasets` module offers datasets with the following format: `(audio, sample_rate, transcription, speaker_id, chapter_id, utterance_id)`.\n",
    "To align with the ESPnet format, we must undertake the following preprocessing steps:\n",
    "\n",
    "- Convert the audio to a `np.ndarray` instance in a single channel.\n",
    "- Tokenize the transcription and convert it to a `np.ndarray` instance.\n",
    "\n",
    "We define a `data_info` argument below to specify these preprocessing steps, which is then provided to the `ESPnetEasyDataset` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.text.build_tokenizer import build_tokenizer\n",
    "from espnet2.text.token_id_converter import TokenIDConverter\n",
    "\n",
    "tokenizer = build_tokenizer(\n",
    "    token_type='bpe',\n",
    "    bpemodel='data/bpemodel/bpe.model'\n",
    ")\n",
    "converter = TokenIDConverter('data/bpemodel/tokens.txt')\n",
    "\n",
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n",
    "\n",
    "data_info = {\n",
    "    \"speech\": lambda d: d[0].squeeze(0).numpy(),\n",
    "    \"text\": lambda d: tokenize(d[2]),\n",
    "}\n",
    "\n",
    "# Convert into ESPnet-Easy dataset format\n",
    "train_dataset = ez.dataset.ESPnetEasyDataset(train_dataset, data_info=data_info)\n",
    "valid_dataset = ez.dataset.ESPnetEasyDataset(valid_dataset, data_info=data_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Process\n",
    "\n",
    "Configuring process is as the same with the other notebook. Please refer to the `libri100.ipynb` notebook for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "While the configuration remains consistent with other notebooks, the instantiation arguments for the Trainer class differ in this case. As we have not generated dump files, we can disregard arguments related to dump files and directly provide the train/valid dataset classes.\n",
    "\n",
    "```\n",
    "trainer = Trainer(\n",
    "    ...\n",
    "    train_dataset=your_train_dataset_instance,\n",
    "    train_dataset=your_valid_dataset_instance,\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import espnetez as ez\n",
    "\n",
    "EXP_DIR = \"exp/small\"\n",
    "STATS_DIR = \"exp/stats\"\n",
    "\n",
    "# load config\n",
    "training_config = ez.config.from_yaml(\n",
    "    \"asr\",\n",
    "    \"config/train_asr_e_branchformer_size256_mlp1024_linear1024_e12_mactrue_edrop0.0_ddrop0.0.yaml\",\n",
    ")\n",
    "preprocessor_config = ez.utils.load_yaml(\"preprocess.yaml\")\n",
    "training_config.update(preprocessor_config)\n",
    "\n",
    "with open(preprocessor_config[\"token_list\"], \"r\") as f:\n",
    "    training_config[\"token_list\"] = [t.replace(\"\\n\", \"\") for t in f.readlines()]\n",
    "\n",
    "# Define the Trainer class\n",
    "trainer = ez.Trainer(\n",
    "    task='asr',\n",
    "    train_config=training_config,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
