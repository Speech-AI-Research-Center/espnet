{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWSM finetuning with custom dataset\n",
    "This Jupyter notebook provides a step-by-step guide on using the ESPnetEasy module to finetune owsm model. In this demonstration, we will leverage the custom dataset to finetune an OWSM model for ASR task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "For this tutorial, we assume that we have the custom dataset with 654 audio with the following directory structure:\n",
    "```\n",
    "audio\n",
    "├── 001 [420 entries exceeds filelimit, not opening dir]\n",
    "└── 002 [234 entries exceeds filelimit, not opening dir]\n",
    "transcription\n",
    "└── owsm_v3.1\n",
    "      ├── 001.csv\n",
    "      └── 002.csv\n",
    "```\n",
    "The csv files contains the audio path, text, and text_ctc data in Japanese. For example, the csv constains the following data:\n",
    "```\n",
    "audio/001/00014.wav,しゃべるたびに追いかけてくるんですけど,なんかしゃべるたびにおいかけてくるんですけど\n",
    "audio/001/00015.wav,え、どうしよう,えどうしよう\n",
    "audio/001/00017.wav,え、何どうしたらなおるの、これ,えなな何どうしたらなおるのこれ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from espnet2.bin.s2t_inference import Speech2Text\n",
    "from espnet2.layers.create_lora_adapter import create_lora_adapter\n",
    "import espnetez as ez\n",
    "\n",
    "# Define hyper parameters\n",
    "DUMP_DIR = f\"./dump\"\n",
    "CSV_DIR = f\"./transcription\"\n",
    "EXP_DIR = f\"./exp/finetune\"\n",
    "STATS_DIR = f\"./exp/stats_finetune\"\n",
    "\n",
    "FINETUNE_MODEL = \"espnet/owsm_v3.1_ebf\"\n",
    "LORA_TARGET = [\n",
    "    \"w_1\", \"w_2\", \"merge_proj\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define the custom dataset class. The owsm finetuning requires `audio`, `text`, `text_prev` and `text_ctc` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        # data_list is a list of tuples (audio_path, text, text_ctc)\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._parse_single_data(self.data[idx])\n",
    "\n",
    "    def _parse_single_data(self, d):\n",
    "        text = f\"<jpn><asr><notimestamps> {d['transcript']}\"\n",
    "        return {\n",
    "            \"audio_path\": d[\"audio_path\"],\n",
    "            \"text\": text,\n",
    "            \"text_prev\": \"<na>\",\n",
    "            \"text_ctc\": d['text_ctc'],\n",
    "        }\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for csv_file in sorted(glob(os.path.join(CSV_DIR, \"*.csv\"))):\n",
    "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list += f.readlines()[1:] # skip header\n",
    "\n",
    "validation_examples = 20\n",
    "train_dataset = CustomDataset(data_list[:-validation_examples])\n",
    "valid_dataset = CustomDataset(data_list[-validation_examples:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training configs and model\n",
    "\n",
    "Since we are going to finetune an OWSM model for ASR task, we will use the tokenizer and TokenIDConverter of the OWSM model. We will also use the training config as the default parameter sets, and update them with the finetuning configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    "    category_sym=\"<jpn>\",\n",
    "    beam_size=10,\n",
    ") # Load model to extract configs.\n",
    "pretrain_config = vars(pretrained_model.s2t_train_args)\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "del pretrained_model\n",
    "\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "\t's2t',\n",
    "\tpretrain_config,\n",
    "\tf\"./config/finetune_with_lora.yaml\"\n",
    ")\n",
    "\n",
    "# define model loading function\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def build_model_fn(args):\n",
    "    pretrained_model = Speech2Text.from_pretrained(\n",
    "        FINETUNE_MODEL,\n",
    "        category_sym=\"<jpn>\",\n",
    "        beam_size=10,\n",
    "    )\n",
    "    model = pretrained_model.s2t_model\n",
    "    model.train()\n",
    "    print(f'Trainable parameters: {count_parameters(model)}')\n",
    "    # apply lora\n",
    "    create_lora_adapter(model, target_modules=LORA_TARGET)\n",
    "    print(f'Trainable parameters after LORA: {count_parameters(model)}')\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap with ESPnetEasyDataset\n",
    "\n",
    "Before initiating the training process, it is crucial to adapt the dataset to the ESPnet format. The dataset class should output tokenized text and audio files in `np.array` format.\n",
    "\n",
    "The `torchaudio.datasets` module offers datasets with the following format: `(audio, sample_rate, transcription, speaker_id, chapter_id, utterance_id)`.\n",
    "To align with the ESPnet format, we must undertake the following preprocessing steps:\n",
    "\n",
    "- Convert the audio to a `np.ndarray` instance in a single channel.\n",
    "- Tokenize the transcription and convert it to a `np.ndarray` instance.\n",
    "\n",
    "We define a `data_info` argument below to specify these preprocessing steps, which is then provided to the `ESPnetEasyDataset` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n",
    "\n",
    "# The output of CustomDatasetInstance[idx] will converted to np.array\n",
    "# with the functions defined in the data_info dictionary.\n",
    "data_info = {\n",
    "    \"speech\": lambda d: librosa.load(d[\"audio_path\"], sr=16000)[0],\n",
    "    \"text\": lambda d: tokenize(d[\"text\"]),\n",
    "    \"text_prev\": lambda d: tokenize(d[\"text_prev\"]),\n",
    "    \"text_ctc\": lambda d: tokenize(d[\"text_ctc\"]),\n",
    "}\n",
    "\n",
    "# Convert into ESPnet-Easy dataset format\n",
    "train_dataset = ez.dataset.ESPnetEasyDataset(train_dataset, data_info=data_info)\n",
    "valid_dataset = ez.dataset.ESPnetEasyDataset(valid_dataset, data_info=data_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "While the configuration remains consistent with other notebooks, the instantiation arguments for the Trainer class differ in this case. As we have not generated dump files, we can disregard arguments related to dump files and directly provide the train/valid dataset classes.\n",
    "\n",
    "```\n",
    "trainer = Trainer(\n",
    "    ...\n",
    "    train_dataset=your_train_dataset_instance,\n",
    "    train_dataset=your_valid_dataset_instance,\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task='s2t',\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    build_model_fn=build_model_fn, # provide the pre-trained model\n",
    "    data_info=data_info,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "When training is done, we can use the inference API to generate the transcription, but don't forget to apply lora before loading the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = Speech2Text.from_pretrained(\n",
    "    \"espnet/owsm_v3.1_ebf\",\n",
    "    category_sym=\"<jpn>\",\n",
    "    beam_size=10,\n",
    "    device=DEVICE\n",
    ")\n",
    "create_lora_adapter(model.s2t_model, target_modules=LORA_TARGET)\n",
    "model.s2t_model.eval()\n",
    "d = torch.load(\"./exp/finetune/1epoch.pth\")\n",
    "model.s2t_model.load_state_dict(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "As a result, the finetuned owsm-v3.1 could successfully transcribe the audio files.\n",
    "\n",
    "**Example**\n",
    "- before finetune: 出してこの時間二のどりを。  \n",
    "- after finetune: ダンスでこの世界に彩りを。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
